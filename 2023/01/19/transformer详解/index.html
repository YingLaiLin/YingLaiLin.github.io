<!DOCTYPE html>

<html lang="zh-CN">

<head>
    
    <title>transformer详解 - 土鸡莱莱的博客</title>
    <meta charset="UTF-8">
    <meta name="keywords" content="摸鱼">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
    
    

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <meta name="description" content="[TOC] Transformer 详解Motivation在Transformer 之前，机器翻译模型大多是基于 RNN 的 encoder-decoer 模型：  然而，RNN 天生存在缺陷，即会存在梯度消失的问题。这是因为只要权重矩阵的特征值 &lt; 1时，其递归相乘就会导致梯度越来越小，从而出现梯度消失的问题。类似地，如果不对矩阵的特征值进行约束，导致其特征值过大，其也会出现梯度爆炸">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer详解">
<meta property="og:url" content="https://yinglailin.github.io/2023/01/19/transformer%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="土鸡莱莱的博客">
<meta property="og:description" content="[TOC] Transformer 详解Motivation在Transformer 之前，机器翻译模型大多是基于 RNN 的 encoder-decoer 模型：  然而，RNN 天生存在缺陷，即会存在梯度消失的问题。这是因为只要权重矩阵的特征值 &lt; 1时，其递归相乘就会导致梯度越来越小，从而出现梯度消失的问题。类似地，如果不对矩阵的特征值进行约束，导致其特征值过大，其也会出现梯度爆炸">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKs0YxBGpVMg6r9R%2Fencoder-decoder.jpg?generation=1583677017316759&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKs385IS1AJ5IPg7%2Frnn-vanishing-gradient.png?generation=1583677008696862&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsAyK8cPrMaMvjC%2Ftransformer-model-architecture.jpg?generation=1583677021368523&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsKWfvSHDX8l-ff%2Fencoder-introduction.jpg?generation=1583677010388256&alt=media">
<meta property="og:image" content="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-00-52-22-image.png">
<meta property="og:image" content="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-01-01-18-image.png">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsRIyR8d4qMnFoq%2Fself-attention-intuiation.jpg?generation=1583677019227629&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKscB0AwYixUBCvd%2Fmulti-head-attention.jpg?generation=1583677014253071&alt=media">
<meta property="og:image" content="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-01-11-06-image.png">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKskeJdSFHl1QHng%2Fmulti-headed-attention-4.jpg?generation=1583677009462458&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsm7hF0Pgr2phbd%2Fmulti-headed-attention-5.jpg?generation=1583677012711592&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsq6fpifvLfg07L%2Fpositional-encoding.jpg?generation=1583677015626130&alt=media">
<meta property="og:image" content="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-26-01-49-19-image.png">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKss30VqNt1oFjA6%2Fpositional-encoding-2.jpg?generation=1583677017150970&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsuJ2YVZ6qITd70%2Fpositional-encoding-3.jpg?generation=1583677018534319&alt=media">
<meta property="og:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKswD_PlsBleHQLz%2Fpositional-encoding-4.jpg?generation=1583677016944420&alt=media">
<meta property="article:published_time" content="2023-01-18T23:55:00.000Z">
<meta property="article:modified_time" content="2023-01-27T09:30:06.217Z">
<meta property="article:author" content="YingLai Lin">
<meta property="article:tag" content="摸鱼">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKs0YxBGpVMg6r9R%2Fencoder-decoder.jpg?generation=1583677017316759&alt=media">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/highlight.js@9.15.8/styles/atom-one-dark.css,npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css,gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css,npm/hexo-theme-nexmoe@latest/source/lib/mdui_043tiny/css/mdui.css,npm/hexo-theme-nexmoe@latest/source/lib/iconfont/iconfont.css?v=233" crossorigin>
    <link rel="stylesheet" href="/css/style.css?v=1675349254814">
     
    
    <link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1675349254814">
    
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="mdui-drawer-body-left">
    
    <div id="nexmoe-background">
        <div class="nexmoe-bg" style="background-image: url(https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg)"></div>
        <div class="mdui-appbar mdui-shadow-0">
            <div class="mdui-toolbar">
                <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
                <div class="mdui-toolbar-spacer"></div>
                <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
                <a href="/" title="YingLai Lin" class="mdui-btn mdui-btn-icon"><img src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/avatar.png" alt="YingLai Lin"></a>
            </div>
        </div>
    </div>
    <div id="nexmoe-header">
        <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="YingLai Lin">
            <img src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/avatar.png" alt="YingLai Lin" alt="YingLai Lin">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>文章</span>40</div>
        <div><span>标签</span>14</div>
        <div><span>分类</span>3</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/" title="回到首页">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                回到首页
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/about.html" title="关于博客">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                关于博客
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple false" href="/PY.html" title="我的朋友">
            <i class="mdui-list-item-icon nexmoefont icon-unorderedlist"></i>
            <div class="mdui-list-item-content">
                我的朋友
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
    
    <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
         
            <form id="search_form" action_e="https://cn.bing.com/search?q=site:nexmoe.com" onsubmit="return search();">
                <label><input id="search_value" name="q" type="search" placeholder="搜索"></label>
            </form>
         
    </div>
</div>
    
    <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="https://space.bilibili.com/20238211" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/YingLaiLin" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a>
    </div>
</div>
    
    
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章分类</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Java-后端/">Java 后端</a>
          <span class="category-list-count">11</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Java-后端/搜索/分布式/">分布式</a>
          <span class="category-list-count">4</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Java-后端/搜索/">搜索</a>
          <span class="category-list-count">4</span>
        </li>

        
      </ul>

    </div>
  </div>


    
    
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/tags/ES/" style="font-size: 20px;">ES</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/JVM/" style="font-size: 13.33px;">JVM</a> <a href="/tags/Java/" style="font-size: 16.67px;">Java</a> <a href="/tags/Java-Exception/" style="font-size: 10px;">Java Exception</a> <a href="/tags/Java-Method-Call/" style="font-size: 10px;">Java Method Call</a> <a href="/tags/classpath/" style="font-size: 10px;">classpath</a> <a href="/tags/github-pages/" style="font-size: 10px;">github pages</a> <a href="/tags/%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB/" style="font-size: 13.33px;">冷热分离</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-mindSpore/" style="font-size: 10px;">分布式训练, 机器学习, mindSpore</a> <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/" style="font-size: 13.33px;">大模型</a> <a href="/tags/%E5%B9%B6%E5%8F%91/" style="font-size: 10px;">并发</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" style="font-size: 16.67px;">数据库</a> <a href="/tags/%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96/" style="font-size: 13.33px;">架构优化</a>
    </div>
    
  </div>

    
</aside>
    <div class="nexmoe-copyright">
        &copy; 2023 YingLai Lin
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        <br><a target="_blank" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"><img src="https://i.dawnlab.me/c0268c1e6cfd0863d6ba35be1575941a.png" width="150px"></a><script data-ad-client="ca-pub-2058306854838448" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    </div>
</div><!-- .nexmoe-drawer -->
    </div>
    <div id="nexmoe-content">
        <div class="nexmoe-primary">
            <div class="nexmoe-post">

  <article>
      
          <div class="nexmoe-post-cover" style="padding-bottom: 66.66666666666666%;"> 
              <img data-src="https://cdn.jsdelivr.net/gh/nexmoe/nexmoe.github.io@latest/images/cover/5c3aec85a4343.jpg" data-sizes="auto" alt="transformer详解" class="lazyload">
              <h1>transformer详解</h1>
          </div>
      
      
      <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2023年01月18日</a>
    <a><i class="nexmoefont icon-areachart"></i>2.2k 字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概 9 分钟</a>
</div>

      

      <span id="more"></span>

<p>[TOC]</p>
<h1 id="Transformer-详解"><a href="#Transformer-详解" class="headerlink" title="Transformer 详解"></a>Transformer 详解</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在Transformer 之前，机器翻译模型大多是基于 RNN 的 encoder-decoer 模型：</p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKs0YxBGpVMg6r9R%2Fencoder-decoder.jpg?generation=1583677017316759&alt=media"></p>
<p>然而，RNN 天生存在缺陷，即会存在<strong>梯度消失</strong>的问题。这是因为只要权重矩阵的特征值 &lt; 1时，其递归相乘就会导致梯度越来越小，从而出现梯度消失的问题。类似地，如果不对矩阵的特征值进行约束，导致其特征值过大，其也会出现<strong>梯度爆炸</strong>。</p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKs385IS1AJ5IPg7%2Frnn-vanishing-gradient.png?generation=1583677008696862&alt=media"></p>
<h3 id="Transformer-为何优于-RNN"><a href="#Transformer-为何优于-RNN" class="headerlink" title="Transformer 为何优于 RNN"></a>Transformer 为何优于 RNN</h3><p>从 RNN 的结构上来说，由于其只能从左向右依次计算，这种机制带来了如下问题：</p>
<ul>
<li><p><strong>并行能力差</strong></p>
<ul>
<li>时间片 t 的计算依赖于 t - 1 时刻的计算结果，从而限制了模型的并行能力</li>
</ul>
</li>
<li><p><strong>长程依赖中的信息丢失</strong></p>
<ul>
<li>顺序计算过程中信息会丢失，尽管 LSTM 等门机制一定程度上缓解了长期依赖的问题，但是对于特别长的依赖，LSTM 依旧无能为力</li>
</ul>
</li>
</ul>
<p>Transformer 的出现解决了上述两个问题：</p>
<ul>
<li><p><strong>Attention 机制</strong>：基于 Attention 机制，将序列中任意两个位置之间的距离缩小为一个常量</p>
</li>
<li><p><strong>结构非顺序依赖</strong>：其结构非类 RNN 的顺序结构，具备更好的并行性，符合现在的 GPU 架构</p>
</li>
</ul>
<h2 id="Transformer-模型架构"><a href="#Transformer-模型架构" class="headerlink" title="Transformer 模型架构"></a>Transformer 模型架构</h2><p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsAyK8cPrMaMvjC%2Ftransformer-model-architecture.jpg?generation=1583677021368523&alt=media"></p>
<p>Transformer模型架构如上图所示，其基于 Encoder-Decoder 框架：</p>
<ul>
<li><p><strong>Encoder</strong></p>
<ul>
<li><p>其输入是单词的 <strong>Embedding</strong> + <strong>Position Embedding</strong>（位置编码）</p>
</li>
<li><p><strong>block</strong>层接受对应的输入， 并且 block 层可以堆叠形成更深的结构</p>
<ul>
<li><p>Attetion 层</p>
</li>
<li><p>FFN （全连接）层</p>
<ul>
<li>通常为 2 层的全连接网络</li>
</ul>
</li>
<li><p>每层的额外处理</p>
<ul>
<li><p>Skip connection </p>
</li>
<li><p>Normalizaiton 层</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Decoder</strong></p>
<ul>
<li><p>第一次输入为<strong>前缀信息</strong>， 之后就是上一层产生的 Embedding + Position Embedding</p>
<ul>
<li><p>这里的前缀信息就是<strong>起始符</strong>(特殊 token），通常用于预测第一个 token。</p>
</li>
<li><p>前缀信息并非 encoder 的输出</p>
</li>
</ul>
</li>
<li><p>block 接受对应的输入， 并且可以堆叠形成更深的结构</p>
<ul>
<li><p>Attetion 层</p>
</li>
<li><p>cross Attention 层</p>
<ul>
<li>比 encoder 多的部分</li>
</ul>
</li>
<li><p>FFN 层</p>
</li>
<li><p>每层的额外处理</p>
<ul>
<li><p>Skip connection</p>
</li>
<li><p>Normalizaiton 层</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>最后输出要通过 Linear （全连接）层，再通过 softmax 做预测</p>
</li>
</ul>
</li>
</ul>
<h3 id="Dive-into-Encoder"><a href="#Dive-into-Encoder" class="headerlink" title="Dive into Encoder"></a>Dive into Encoder</h3><p>以单层 transformer 为例，其 encoder 结构图如下所示：</p>
<img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsKWfvSHDX8l-ff%2Fencoder-introduction.jpg?generation=1583677010388256&alt=media" title alt width="589">

<p>其输入包含 2 个单词(token) 对应的 embedding， 分别为 $x_1$ 和 $x_2$， 其分别经过一个 self-Attention 层和全连接网络层，最后得到输出分别为 $r_1$ 和 $r_2$。其中，self-Attention 中所有输入向量都参与了该过程，进行了<strong>信息交换</strong>并得到中间变量 $z_1$ 和 $z_2$。</p>
<p>而 FFN 层中单词是分别进行转换的，即$r_1 = FFN(z_1), r_2 = FFN(z_2)$</p>
<h4 id="self-attention-机制"><a href="#self-attention-机制" class="headerlink" title="self-attention 机制"></a>self-attention 机制</h4><p>在 self-attention 中，其存在三个矩阵 $W^Q, W^K, W^v$,  这三个矩阵分别与输入 token 的 embedding 相乘，得到对应的 $q, k, v$,  如对于 $x_1, x_2$, 有：</p>
<p>$q_1 = x_1 W^q , k_1 = x_1W^K, v_1 = x_1 W^v$, </p>
<p>$q_2 = x_2 W^q ,  k_2 = x_2W^K, v_2 = x_2 W^v$</p>
<p>通过线性组合就可以得到对应的 $z_1, z_2$:</p>
<p>$z_1 = \theta_{11} v_1 + \theta_{12} v_2$</p>
<p>$z_2 = \theta_{21} v_1 + \theta_{22} v_2$</p>
<p>其中,  $\theta$ 基于 softmax 加权得到:</p>
<img src="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-00-52-22-image.png" title alt width="288">

<p>这里的 $\theta$ 就是线性变换，相比于单层 FFN 只是缺少了非线性变化。即在这层，对不同的 tokens 进行了融合(至于为什么相加就是融合，这算一种 trick 吧)。</p>
<p>从整体上来说，$\theta_{n} $ 代表 $q_n$  与 不同 $key $ 经过 softmax 归一化的相似性，作为 $value$ 的权重，也就是说，这里实际上就是在做关于 $value$ 的加权计算， 只不过 $q，k，v $都是基于同一个向量 $x_n$变换得到的。</p>
<h6 id="理解-q-k-v-向量"><a href="#理解-q-k-v-向量" class="headerlink" title="理解 $q, k ,v$ 向量"></a>理解 $q, k ,v$ 向量</h6><p>$q, k ,v$ 向量的思路来自于较早的信息检索领域，其中 $q$ 即 query，$k$ 即 key，$v$ 即值，而$(k,v)$ 就是键值对, 即使用 query 关键词去找到最相关的检索结果。</p>
<p>举个列子， 假设 query 是 5G， 而 (k-v) 有：</p>
<ul>
<li><p>(5G, Huawei)</p>
</li>
<li><p>(4G, Nokia)</p>
</li>
</ul>
<p>因此，softmax 就是在计算 $q$ 与 $k$ 的相似度，最后对 $v$ 进行加权</p>
<h6 id="self-attention-的矩阵形式"><a href="#self-attention-的矩阵形式" class="headerlink" title="self-attention 的矩阵形式"></a>self-attention 的矩阵形式</h6><p>为了得到query，key，value，每个 token 都需要做 3次乘法，则 n 个 token 就需要 3n 次乘法。了比较高效的实现矩阵乘法，要进行类似matlab中的向量化操作，将上述操作转换为矩阵形式：</p>
<img title src="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-01-01-18-image.png" alt width="478">

<p><strong>注意点</strong></p>
<ul>
<li>$d_k​$ 是向量$q, k$ 的维度，这两个向量的维度相同，因为两者要做点积。但是</li>
</ul>
<p>$v$ 的维度并不一定等于 $d_k$.</p>
<ul>
<li>这里除以 $\sqrt d_k$ 是为了防止维度过高时， $QK^T$ 的值过大导致 softmax 反向传播时发生梯度消失。$\sqrt d_k$是经验值，这是因为从理论上来说，$QK^T$ 值需要适度增大，而又不能过度增加。</li>
</ul>
<h6 id="直观理解-self-attention"><a href="#直观理解-self-attention" class="headerlink" title="直观理解 self-attention"></a>直观理解 self-attention</h6><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsRIyR8d4qMnFoq%2Fself-attention-intuiation.jpg?generation=1583677019227629&alt=media" title alt width="355">

<p>如上图所示， 假设句子的输入$x_1, x_2, \dots, x_{14}$ ，则 $z_1$ 是对应输入的线性组合构造的，即单个句子内的单词互相查看自己的权重后进行加权，如上所示， 对于单词 <em>it(即$z</em>{10}$) ，其为 14 个单词的线性组合。其中 the 和 animal 与其的关联程度最高，即当前词的注意力更集中在句子中的这两个词上。</p>
<h6 id="为什么-q-k-v-可以基于矩阵乘法得到"><a href="#为什么-q-k-v-可以基于矩阵乘法得到" class="headerlink" title="为什么 $q, k, v$ 可以基于矩阵乘法得到"></a>为什么 $q, k, v$ 可以基于矩阵乘法得到</h6><h5 id="multi-head-Attention"><a href="#multi-head-Attention" class="headerlink" title="multi-head Attention"></a>multi-head Attention</h5><p>通过设置多个 $W^Q, W^K, W^v$ 矩阵，就能够得到不同的 $Q, K, V$，从而能够使得 <strong>Attention 有更丰富的层次</strong>。也就是说，<strong>不同的 self-attention head 代表不同方面的理解</strong>。</p>
<img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKscB0AwYixUBCvd%2Fmulti-head-attention.jpg?generation=1583677014253071&alt=media" title alt width="625">

<p>接着，由于存在多个 中间变量$z_1, z_2, \dots, z_n$ ，就需要使用 concat 聚合这些变量，多个版本的 z 拼接为长向量，再使用一个全连接神经网络获取一个较短的最终向量$z^{‘}$：</p>
<img title src="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-20-01-11-06-image.png" alt width="505">

<img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKskeJdSFHl1QHng%2Fmulti-headed-attention-4.jpg?generation=1583677009462458&alt=media" title alt width="609">

<h5 id="multi-head-attention-的直观理解"><a href="#multi-head-attention-的直观理解" class="headerlink" title="multi-head attention 的直观理解"></a>multi-head attention 的直观理解</h5><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsm7hF0Pgr2phbd%2Fmulti-headed-attention-5.jpg?generation=1583677012711592&alt=media" title alt width="664">

<p>上图中存在八个Attention，以八种不同的颜色表示。以左边为例，即橙色部分，单词<code>it</code>连接的权重最重的是<code>animal</code>，这是从某一个方面来看。从另一方面来看，即绿色部分，<code>it</code>最关注的是<code>tired</code>。橙色的注意力主要表明<code>it</code>是个什么东西，从东西的角度说明它是一种动物，而不是苹果或者香蕉。如果我们从状态这个层面来看，<code>it</code>这个动物现在是在怎么样的一个状态，它的状态是<code>tired</code>，而不是兴奋。所以<strong>不同的Self-Attention Head是不同方面的理解</strong>。</p>
<h6 id="self-attention-与-attention-的区别"><a href="#self-attention-与-attention-的区别" class="headerlink" title="self-attention 与 attention 的区别"></a>self-attention 与 attention 的区别</h6><p>以Encoder-Decoder框架为例，其输入 Source 和 输出 Target 的内容是不一样的,比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子, 两者具体计算过程是一样的，只是计算对象发生了变化而已: </p>
<ul>
<li><p><strong>Attention</strong>: Attention发生在Target的元素 Query和Source中的所有元素之间。</p>
</li>
<li><p><strong>Self Attention</strong>: Source 内部元素之间或者 Target内部元素之间发生的 Attention机制，也可以理解为Target = Source 这种<strong>特殊情况下的Attention</strong>。</p>
</li>
</ul>
<h4 id="Encoder-输入"><a href="#Encoder-输入" class="headerlink" title="Encoder 输入"></a>Encoder 输入</h4><p>Embedding = Word Embedding + Positional Embedding，如下图所示，接下来将分别介绍这两部分：</p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsq6fpifvLfg07L%2Fpositional-encoding.jpg?generation=1583677015626130&alt=media"></p>
<h5 id="词向量-Embedding-输入"><a href="#词向量-Embedding-输入" class="headerlink" title="词向量 Embedding 输入"></a>词向量 Embedding 输入</h5><p><strong>Encoder</strong> 输入的是单词 $x$ 的embedding，通常有两种选择：</p>
<ul>
<li><p><strong>预训练 embedding</strong></p>
<ul>
<li><p>使用Pre-trained的<strong>embeddings并固化</strong></p>
</li>
<li><p>使用Pre-trained的<strong>embeddings</strong>, 但参数设为 trainable</p>
</li>
</ul>
</li>
<li><p><strong>随机初始化</strong>（Transformer 选择）</p>
<ul>
<li>对其进行随机初始化，但<strong>设为Trainable</strong>，即End2End训练方式。</li>
</ul>
</li>
</ul>
<h5 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h5><p><strong>为什么需要位置编码</strong></p>
<p>由于 Transformer模型没有用RNN也没有卷积，所以为了让模型能<strong>利用序列的顺序</strong>，必须输入序列中词的位置。这些位置编码和输入向量 $x$ 维度相同，可以通过两者相加，从而将位置信息注入</p>
<p><strong>位置编码的衡量</strong></p>
<p>位置是相对的，也就是需要一个坐标系来衡量任意点之间的位置。这里，使用如下函数进行距离的衡量：</p>
<img src="file:///Users/leon/Library/Application%20Support/marktext/images/2023-01-26-01-49-19-image.png" title alt width="326">

<p><strong>举个例子：</strong></p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKss30VqNt1oFjA6%2Fpositional-encoding-2.jpg?generation=1583677017150970&alt=media"></p>
<p>下图为一个长度为50，维度是128的句子的Positional Encoding（每一行为一个Encoding向量）。下图中一行就是一个单词的Positional Encoding: – 不理解</p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKsuJ2YVZ6qITd70%2Fpositional-encoding-3.jpg?generation=1583677018534319&alt=media"></p>
<p>Positional Encoding的<strong>物理意义</strong>是：把50个Positional Encoding两两互相做点击，看相关性。<strong>其特点是Encoding向量的点积值对称，随着距离增大而减小</strong>。</p>
<p><img src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKswD_PlsBleHQLz%2Fpositional-encoding-4.jpg?generation=1583677016944420&alt=media"></p>
<h3 id="Dive-into-Decoder"><a href="#Dive-into-Decoder" class="headerlink" title="Dive into Decoder"></a>Dive into Decoder</h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/344516091/answer/899804534">Transformer模型中，decoder的第一个输入是什么？ - 既安的回答 - 知乎</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer">Self-Attention和Transformer - machine-learning-notes</a>)</p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166608727">举个例子讲下transformer的输入输出细节及其他 - 隔壁大王的文章 - 知乎</a></p>

  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>本文作者：</strong>YingLai Lin<br>
        <strong>本文链接：</strong><a href="https://yinglailin.github.io/2023/01/19/transformer%E8%AF%A6%E8%A7%A3/" title="https:&#x2F;&#x2F;yinglailin.github.io&#x2F;2023&#x2F;01&#x2F;19&#x2F;transformer%E8%AF%A6%E8%A7%A3&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;yinglailin.github.io&#x2F;2023&#x2F;01&#x2F;19&#x2F;transformer%E8%AF%A6%E8%A7%A3&#x2F;</a><br>
        
            <strong>版权声明：</strong>本文采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 协议进行许可
        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
    
    
</div>

  
      <div class="nexmoe-post-footer">
          <section class="nexmoe-comment">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.css">
<div id="gitalk"></div>
<script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script>
<script type="text/javascript">
    var gitalk = new Gitalk({
        clientID: '80b2453b6d5f37ad6225',
        clientSecret: '43e99fa852795c9a7b3eb924b2558c64b84bbdeb',
        id: window.location.pathname,
        repo: 'https://yinglailin.github.io/',
        owner: 'yinglai',
        admin: 'yinglai'
    })
    gitalk.render('gitalk')
</script>
</section>
      </div>
  
</div>
            <div class="nexmoe-post-right">
              <div class="nexmoe-fixed">
                  <div class="nexmoe-tool"> 
                    
                      
                        
                          
                          
                              <button class="mdui-fab catalog" style="overflow:unset;">
                                  <i class="nexmoefont icon-i-catalog"></i>
                                  <div class="nexmoe-toc">
                                      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer-%E8%AF%A6%E8%A7%A3"><span class="toc-number">1.</span> <span class="toc-text">Transformer 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivation"><span class="toc-number">1.1.</span> <span class="toc-text">Motivation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-%E4%B8%BA%E4%BD%95%E4%BC%98%E4%BA%8E-RNN"><span class="toc-number">1.1.1.</span> <span class="toc-text">Transformer 为何优于 RNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.2.</span> <span class="toc-text">Transformer 模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dive-into-Encoder"><span class="toc-number">1.2.1.</span> <span class="toc-text">Dive into Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention-%E6%9C%BA%E5%88%B6"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">self-attention 机制</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%90%86%E8%A7%A3-q-k-v-%E5%90%91%E9%87%8F"><span class="toc-number">1.2.1.1.0.1.</span> <span class="toc-text">理解 $q, k ,v$ 向量</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#self-attention-%E7%9A%84%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.2.1.1.0.2.</span> <span class="toc-text">self-attention 的矩阵形式</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3-self-attention"><span class="toc-number">1.2.1.1.0.3.</span> <span class="toc-text">直观理解 self-attention</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-q-k-v-%E5%8F%AF%E4%BB%A5%E5%9F%BA%E4%BA%8E%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%BE%97%E5%88%B0"><span class="toc-number">1.2.1.1.0.4.</span> <span class="toc-text">为什么 $q, k, v$ 可以基于矩阵乘法得到</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#multi-head-Attention"><span class="toc-number">1.2.1.1.1.</span> <span class="toc-text">multi-head Attention</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#multi-head-attention-%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">1.2.1.1.2.</span> <span class="toc-text">multi-head attention 的直观理解</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#self-attention-%E4%B8%8E-attention-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.2.1.1.2.1.</span> <span class="toc-text">self-attention 与 attention 的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Encoder-%E8%BE%93%E5%85%A5"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Encoder 输入</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F-Embedding-%E8%BE%93%E5%85%A5"><span class="toc-number">1.2.1.2.1.</span> <span class="toc-text">词向量 Embedding 输入</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.2.1.2.2.</span> <span class="toc-text">位置编码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dive-into-Decoder"><span class="toc-number">1.2.2.</span> <span class="toc-text">Dive into Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.3.</span> <span class="toc-text">参考资料</span></a></li></ol></li></ol>
                                  </div>
                              </button>
                          
                          
                      
                    
                      <a href="#nexmoe-content" class="toc-link" aria-label="回到顶部" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
                  </div>
              </div>
            </div>
        </div>
    </div>
     
    <div id="nexmoe-search-space">
        <div class="search-container">
            <div class="search-header">
                <div class="search-input-container">
                    <input class="search-input" type="text" placeholder="搜索" oninput="sinput();">
                </div>
                <a class="search-close" onclick="sclose();">×</a>
            </div>
            <div class="search-body"></div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js?v=1"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

 

<script async src="/js/app.js?v=1675349254820"></script>



<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>


    





<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":200,"height":500},"mobile":{"show":true},"log":false});</script></body>

</html>
